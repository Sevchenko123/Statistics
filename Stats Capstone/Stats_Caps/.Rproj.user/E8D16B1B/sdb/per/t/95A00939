{
    "collab_server" : "",
    "contents" : "---\ntitle: \"Peer Assessment II\"\noutput:\n  html_document: \n    pandoc_args: [\n      \"--number-sections\",\n    ]\n---\n\n# Background\n\nAs a statistical consultant working for a real estate investment firm, your task is to develop a model to predict the selling price of a given home in Ames, Iowa. Your employer hopes to use this information to help assess whether the asking price of a house is higher or lower than the true value of the house. If the home is undervalued, it may be a good investment for the firm.\n\n# Training Data and relevant packages\n\nIn order to better assess the quality of the model you will produce, the data have been randomly divided into three separate pieces: a training data set, a testing data set, and a validation data set. For now we will load the training data set, the others will be loaded and used later.\n\n```{r load, message = FALSE}\nload(\"ames_train.Rdata\")\n```\n\nUse the code block below to load any necessary packages\n\n```{r packages, message = FALSE}\nlibrary(statsr)\nlibrary(dplyr)\nlibrary(BAS)\nlibrary(ggplot2)\nlibrary(GGally)\nlibrary(leaps)\nlibrary(MASS)\n```\n\n## Part 1 - Exploratory Data Analysis (EDA)\n\nWhen you first get your data, it's very tempting to immediately begin fitting models and assessing how they perform.  However, before you begin modeling, it's absolutely essential to explore the structure of the data and the relationships between the variables in the data set.\n\nDo a detailed EDA of the ames_train data set, to learn about the structure of the data and the relationships between the variables in the data set (refer to Introduction to Probability and Data, Week 2, for a reminder about EDA if needed). Your EDA should involve creating and reviewing many plots/graphs and considering the patterns and relationships you see. \n\nAfter you have explored completely, submit the three graphs/plots that you found most informative during your EDA process, and briefly explain what you learned from each (why you found each informative).\n\n* * *\n\n**1. The first plot shows a boxplot of Prices as per the neighborhood. Based on the summary statistics above the least expensive neighborhood is MeadowV with median price of 85,750 dollars. The most expensive neighborhood is StoneBr with median price of 340691.50 dollars. And finally the most heterogeneous neighborhood is also StoneBr with the highest standard deviation of 123459.10 dollars.**\n\n**2. The second plot is a GGpairs plot which shows correlation between price and other variables namely - Overall Quality, Area, Lot Area, Bedroom Above Ground, Half Bath, Full Bath, Total Basement Squarefoot, 1st floor Squarefoot, 2nd floor Squarefoot, Garage Area. The price variable has the strongest correlation with Overall Condition and Area. It has fairly good correlation with Total Basement Squarefoot, 1st Floor Squarefoot, Full bath, Garage Area and weak correlation with variables like Lot area, 2nd floor Squarefoot, Half bath, Bedroom Above Ground. It is interesting to note that is has negative correlation with Overall Condition.**\n\n**3. The third plot shows the boxplot of price based on the kitchen quality of the house. As expected, the price is high if the quality is relatively good. There are only two houses with poor kitchen quality. **\n\n```{r fig.width=17, fig.height=10 ,creategraphs,warning=FALSE}\nggplot(ames_train,aes(x=Neighborhood,y=price))+geom_boxplot(color='black',fill='red',outlier.shape=NA)+theme_bw()+labs(y=\"Prices\",x=\"Neighborhood\") # Neighborhood Boxplot\n\nmy_fn <- function(data, mapping, ...){\n  p <- ggplot(data = data, mapping = mapping) + \n    geom_point(pch=19,alpha=0.5,color='blue') + \n    geom_smooth(method=lm, fill=\"red\", color=\"red\", se=FALSE, ...) +\n    theme_bw()\n  p\n}\ng = ggpairs(ames_train,columns = c(3,2,7,20,21,41,46,47,51,52,53,64), lower = list(continuous = my_fn))\ng # GGPairs plot\n\nggplot(ames_train,aes(x=Kitchen.Qual,y=price))+geom_boxplot(color=c('black','blue','red','brown','darkgreen'))+theme_minimal()+labs(x=\"Kitchen Quality\",y=\"Price\") # Kitchen Quality Boxplot\n```\n\n* * *\n\n## Part 2 - Development and assessment of an initial model, following a semi-guided process of analysis\n\n### Section 2.1 An Initial Model\nIn building a model, it is often useful to start by creating a simple, intuitive initial model based on the results of the exploratory data analysis. (Note: The goal at this stage is **not** to identify the \"best\" possible model but rather to choose a reasonable and understandable starting point. Later you will expand and revise this model to create your final model.\n\nBased on your EDA, select *at most* 10 predictor variables from “ames_train” and create a linear model for `price` (or a transformed version of price) using those variables. Provide the *R code* and the *summary output table* for your model, a *brief justification* for the variables you have chosen, and a *brief discussion* of the model results in context (focused on the variables that appear to be important predictors and how they relate to sales price).\n\n* * *\n\n**Based on the GGPairs plot I have used the variables which show strong or relatively good correlation with the Price variable. The model is a god fit with adjusted R squared of 0.8174.** \n\n```{r fit_model}\nmodel <- lm(log(price)~log(Overall.Qual+1)+log(area+1)+log(Total.Bsmt.SF+1)+log(X1st.Flr.SF+1)+log(Full.Bath+1)+log(Garage.Area+1)+log(Lot.Area+1)+log(X2nd.Flr.SF+1)+log(Half.Bath+1)+log(Bedroom.AbvGr+1),data=ames_train)\nsummary(model)\n```\n\n* * *\n\n### Section 2.2 Model Selection\n\nNow either using `BAS` another stepwise selection procedure choose the \"best\" model you can, using your initial model as your starting point. Try at least two different model selection methods and compare their results. Do they both arrive at the same model or do they disagree? What do you think this means?\n\n* * *\n\n**I have created two more models in addition to above model. The first one using AIC criterion using both backward and forward elimination technique to find a parsimonious model. The final selected model has adjusted R square of 0.8175 which is almost negligible improvement on the previous model. The other model which I created is a Bayesian Regression model. I also calculated the RMSE of all the three models in the later section. All the models arrive with same variables which mean the variables included in the models are important. This is also evident from the p values of the earlier model which tells that all the variables are significant. ** \n\n```{r model_select}\nmodel.AIC <- stepAIC(model,direction=\"both\",k=2)\nsummary(model.AIC) # model based on AIC criterion\n\nmodel.bas <- bas.lm(log(price)~log(Overall.Qual+1)+log(area+1)+log(Total.Bsmt.SF+1)+log(X1st.Flr.SF+1)+log(Full.Bath+1)+log(Garage.Area+1)+log(Lot.Area+1)+log(X2nd.Flr.SF+1)+log(Half.Bath+1)+log(Bedroom.AbvGr+1),data = ames_train, prior = \"AIC\", modelprior=uniform()) # Bayesian Regression Model\n```\n\n* * *\n\n### Section 2.3 Initial Model Residuals\nOne way to assess the performance of a model is to examine the model's residuals. In the space below, create a residual plot for your preferred model from above and use it to assess whether your model appears to fit the data well. Comment on any interesting structure in the residual plot (trend, outliers, etc.) and briefly discuss potential implications it may have for your model and inference / prediction you might produce.\n\n* * *\n\n**I have selected the model based on AIC criterion as it has better adjusted R squared of 0.8175. The following plots show the residual plots. There appears to be random scatter of residuals around 0 except for few outliers. The histogram of residuals also look normally distributed with exception of few outliers. Finally, the last plot shows qq plot which shows the plot of theoretical quantiles to sample quantiles which falls pretty well on the reference line. All these plots therefore shows that the model is performing well.**\n\n```{r model_resid}\nggplot(data = model.AIC, aes(x = .fitted, y = .resid)) +\n  geom_point(color=\"red\",pch=19) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  xlab(\"Fitted values\") +\n  ylab(\"Residuals\")+theme_bw()\n\nggplot(data = model.AIC, aes(x = .resid)) +\n  geom_histogram(color=\"white\",fill=\"red\") +\n  xlab(\"Residuals\")+theme_bw()\n\nqqnorm(model.AIC$residuals,col=\"Red\")\nqqline(model.AIC$residuals,col=\"black\")\n```\n\n* * *\n\n### Section 2.4 Initial Model RMSE\n\nYou can calculate it directly based on the model output. Be specific about the units of your RMSE (depending on whether you transformed your response variable). The value you report will be more meaningful if it is in the original units (dollars).\n\n* * *\n\n**I calculated the RMSE of all the three models on the train data. The RMSE of the first model is 35513.59 dollars which is the best out of all the three models. The RMSE of second model which is based on AIC criterion is 35471.76 dollars. Finally the Bayesion regression model performs the worst out of all with RMSE of 83301.52 dollars.**\n\n\n```{r model_rmse}\nload(\"ames_train.Rdata\")\n\npred.test.model <- predict(model,newdata=ames_train)\npred.test.model[is.na(pred.test.model)] <- 0\npred.rmse.model <- sqrt(mean((exp(pred.test.model)-ames_train$price)^2))\npred.rmse.model # RMSE of First model\n\npred.test.modelaic <- predict(model.AIC,newdata=ames_train)\npred.test.modelaic[is.na(pred.test.modelaic)] <- 0\npred.rmse.modelaic <- sqrt(mean((exp(pred.test.modelaic)-ames_train$price)^2))\npred.rmse.modelaic # RMSE of Model based on AIC criterion\n\npred.test.HPM <- predict(model.bas, newdata = ames_train, estimator=\"HPM\")\npred.HPM.rmse <- sqrt(mean((exp(pred.test.HPM$fit) - ames_train$price)^2))\npred.HPM.rmse\n```\n\n* * *\n\n### Section 2.5 Overfitting \n\nThe process of building a model generally involves starting with an initial model (as you have done above), identifying its shortcomings, and adapting the model accordingly. This process may be repeated several times until the model fits the data reasonably well. However, the model may do well on training data but perform poorly out-of-sample (meaning, on a dataset other than the original training data) because the model is overly-tuned to specifically fit the training data. This is called “overfitting.” To determine whether overfitting is occurring on a model, compare the performance of a model on both in-sample and out-of-sample data sets. To look at performance of your initial model on out-of-sample data, you will use the data set `ames_test`.\n\n```{r loadtest, message = FALSE}\nload(\"ames_test.Rdata\")\n```\n\nUse your model from above to generate predictions for the housing prices in the test data set.  Are the predictions significantly more accurate (compared to the actual sales prices) for the training data than the test data?  Why or why not? Briefly explain how you determined that (what steps or processes did you use)?\n\n* * *\n\n**The RMSE on the test data is bit more than the train data. But evidence suggests that the model fits much better on train data than test data.**\n\n```{r initmodel_test}\nfinal_predictions <- predict(model.AIC,ames_test,interval = \"prediction\",level = 0.95)\npred.final <- sqrt(mean((exp(final_predictions) - ames_test$price)^2))\npred.final\n```\n\n* * *\n\n**Note to the learner:** If in real-life practice this out-of-sample analysis shows evidence that the training data fits your model a lot better than the test data, it is probably a good idea to go back and revise the model (usually by simplifying the model) to reduce this overfitting. For simplicity, we do not ask you to do this on the assignment, however.\n\n## Part 3 Development of a Final Model\n\nNow that you have developed an initial model to use as a baseline, create a final model with *at most* 20 variables to predict housing prices in Ames, IA, selecting from the full array of variables in the dataset and using any of the tools that we introduced in this specialization.  \n\nCarefully document the process that you used to come up with your final model, so that you can answer the questions below.\n\n### Section 3.1 Final Model\n\nProvide the summary table for your model.\n\n* * *\n\n\n```{r model_playground}\nfinal_model <- lm(log(price)~log(Overall.Qual+1)+log(Overall.Cond+1)+log(area+1)+log(Total.Bsmt.SF+1)+log(X1st.Flr.SF+1)+log(Full.Bath+1)+log(Garage.Area+1)+log(Lot.Area+1)+log(X2nd.Flr.SF+1)+log(Half.Bath+1)+log(Bedroom.AbvGr+1)+Heating.QC+Kitchen.Qual+Garage.Qual+Bsmt.Qual+Neighborhood+Central.Air+log(Year.Built)+log(TotRms.AbvGrd)+log(Year.Remod.Add),data=ames_train)\nsummary(final_model)\n```\n\n* * *\n\n### Section 3.2 Transformation\n\nDid you decide to transform any variables?  Why or why not? Explain in a few sentences.\n\n**I transformed all the independent variables using log transformation. Log transformations are used to make highly skewed data less skewed and more normally distributed. I added 1 so as to avoid Log 0 = -infinity.**\n\n* * *\n\n\n```{r model_assess}\n```\n\n* * *\n\n### Section 3.3 Variable Interaction\n\nDid you decide to include any variable interactions? Why or why not? Explain in a few sentences.\n\n**No I did not decide to include any variable interactions.**\n\n* * *\n\n```{r model_inter}\n```\n\n* * *\n\n### Section 3.4 Variable Selection\n\nWhat method did you use to select the variables you included? Why did you select the method you used? Explain in a few sentences.\n\n**Based on the exploratory data analysis I chose the above variables. I included all the variables in the initial model which I built, as the p values show that all the variables are significant in predicting the house price. In addition to those variables I also added some more variables in the model and the model improved as the adjusted R square rose from 0.81 to approximately 0.90. Also if you do a forward selection method on the above full model based on the AIC criterion, the final model contains all the variable and it does not exclude any of the variables.**  \n\n* * *\n\n\n\n```{r}\nm1 <- stepAIC(final_model,direction=\"forward\",k=2) # AIC criterion \n```\n\n* * *\n\n### Section 3.5 Model Testing\n\nHow did testing the model on out-of-sample data affect whether or how you changed your model? Explain in a few sentences.\n\n* * *\n\n**The adjusted R squared of this model has increased from the initial model from 0.8174 to 0.8963. This model is performing better than the previous model built on the test data as the RMSE is 61406.27 dollars as compared to RMSE of 67084.22 dollars in the initial model.**\n\n```{r model_testing}\nfinal_preds <- predict(final_model,ames_test,interval = \"prediction\",level = 0.95)\nfinal_preds[is.na(final_preds)] <- 0\npreds.final <- sqrt(mean((exp(final_preds) - ames_test$price)^2))\npreds.final\n```\n\n* * *\n\n## Part 4 Final Model Assessment\n\n### Section 4.1 Final Model Residual\n\n```{r}\nggplot(data = final_model, aes(x = .fitted, y = .resid)) +\n  geom_point(color=\"blue\",pch=19) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  xlab(\"Fitted values\") +\n  ylab(\"Residuals\")+theme_bw()\n\nggplot(data = final_model, aes(x = .resid)) +\n  geom_histogram(color=\"white\",fill=\"blue\",bins=30) +\n  xlab(\"Residuals\")+theme_bw()\n\nqqnorm(final_model$residuals,col=\"Blue\")\nqqline(final_model$residuals,col=\"black\")\n```\n\n* * *\n\n**The model performs well, the scatter plot shows random scatter of residuals around 0. The histogram of residuals show normally distributed residuals. The qq plot compares the two probability distributions. The points in the Q–Q plot  approximately lie on the reference line which shows the two distributions (predicted values and actual values) are quite similar. All the diagnostic plots show that the model is a good fit. The adjusted R squared of the model is approximately 0.90 which has improved from the previous model.**\n\n* * *\n\n### Section 4.2 Final Model RMSE\n\nFor your final model, calculate and briefly comment on the RMSE.\n\n* * *\n\n**As mentioned before, the model performs pretty well with RMSE of 61406.27 dollars.**\n\n* * *\n\n### Section 4.3 Final Model Evaluation\n\nWhat are some strengths and weaknesses of your model?\n\n* * *\n\n**Strengths include that the model is a very good fit as per the adjusted R squared with adj. R squared of almost 0.90. Weakness of the model could be that it seems to have overfitted as the RMSE on the test data is more than RMSE than train data. But evidence shows that the model fits on training data a lot better than the test data.**\n\n* * *\n\n### Section 4.4 Final Model Validation\n\nTesting your final model on a separate, validation data set is a great way to determine how your model will perform in real-life practice. \n\nYou will use the “ames_validation” dataset to do some additional assessment of your final model. Discuss your findings, be sure to mention:\n* What is the RMSE of your final model when applied to the validation data?  \n* How does this value compare to that of the training data and/or testing data?\n* What percentage of the 95% predictive confidence (or credible) intervals contain the true price of the house in the validation data set?  \n* From this result, does your final model properly reflect uncertainty?\n\n```{r loadvalidation, message = FALSE}\nload(\"ames_validation.Rdata\")\names_validation <- ames_validation %>% filter(Neighborhood != \"Landmrk\")\n```\n\n* * *\n\nNOTE: Write your written response to section 4.4 here. Delete this note before you submit your work.\n\n```{r model_validate}\n# RMSE\nfinal_preds_val <- predict(final_model,ames_validation,interval=\"prediction\",level=0.95)\nfinal_preds_val[is.na(final_preds_val)] <- 0\npred.final.val <- sqrt(mean((exp(final_preds_val) - ames_validation$price)^2))\npred.final.val\n\n# Coverage Probability\npredict.full <- exp(predict(final_model, ames_validation, interval = \"prediction\"))\npredict.full[is.na(predict.full)] <- 0\ncoverage.prob.full <- mean(ames_validation$price > predict.full[,\"lwr\"] &\n                            ames_validation$price < predict.full[,\"upr\"])\ncoverage.prob.full\n```\n\n* * *\n\n## Part 5 Conclusion\n\nProvide a brief summary of your results, and a brief discussion of what you have learned about the data and your model. \n\n* * *\n\n**The model performs better on validation set. The RMSE of model on validation test is 59179.66 dollars which is better than RMSE on test set which is 61406.27 dollars. It still is not better as compared to training set. Almost 89% of the 95% predictive confidence intervals contain the true price of the house in the validation data set. So the model can deal very well with uncertainties. So overall the final model is a better model as compared to the initial model, as it has improved adjusted R squared of almost 0.90 and low RMSE than the initial model. The residual plots also confirm that the final model is a good fit. **\n\n* * *\n",
    "created" : 1487023574211.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3882734148",
    "id" : "95A00939",
    "lastKnownWriteTime" : 1487027778,
    "last_content_update" : 1487027778704,
    "path" : "~/Desktop/stats/Stats Capstone/Final.Rmd",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 2,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}